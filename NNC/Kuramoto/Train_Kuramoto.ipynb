{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b51e6297-25c0-4923-a0f2-147898223f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'device': 0, 'total_memory': 24564.0, 'free_memory': 20583.875, 'used_memory': 3980.125}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..')) \n",
    "sys.path.append(project_root)  # Add folder_a to sys.path\n",
    "sys.path.append(os.path.join(project_root, '../..', 'Networks'))  # Add folder_b to sys.path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim.swa_utils as swa_utils\n",
    "\n",
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "from torch import profiler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import pdb\n",
    "import torchsde\n",
    "from Networks.utils import get_gpu_memory,objective,update_ema, adj_lattice, criticalK, order_param,  load_clean_state_dict, sample_kuramoto,loss_compute_kur,save_model\n",
    "from helpers import  KuramotoHyperUniversal,  find_params\n",
    "import psutil\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import gc\n",
    "from IPython.display import clear_output\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if is_cuda else 'cpu'\n",
    "#device = 'cpu'\n",
    "if not is_cuda:\n",
    "    print(\"Warning: CUDA not available; falling back to CPU but this is likely to be very slow.\")\n",
    "gc.collect()  \n",
    "torch.cuda.empty_cache()\n",
    "path=os.path.abspath(os.getcwd())\n",
    "print(get_gpu_memory())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a6f115-26e4-4762-acec-46b4093d5c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'device': 0, 'total_memory': 24564.0, 'free_memory': 20376.1875, 'used_memory': 4187.8125}]\n",
      "0.841310456374586\n"
     ]
    }
   ],
   "source": [
    "#state size\n",
    "state_size=64\n",
    "#batch size\n",
    "batch_size=1024\n",
    "#weight\n",
    "weight=10**(-4)\n",
    "#scale of frequencies\n",
    "scale=5\n",
    "#scale of initial phases\n",
    "scale_kin=5\n",
    "#learning rate\n",
    "learning_rate=0.0001\n",
    "#scheduler decay rate\n",
    "gamma=0.999\n",
    "#ema coefficient\n",
    "alpha=0.995\n",
    "#generate graph\n",
    "Astored=adj_lattice(state_size, \"erdos\", 0.3)\n",
    "\n",
    "#calculate critical coupling and laplacian\n",
    "Kcrit, L=criticalK(Astored,state_size, scale)\n",
    "#set coupling constant in experiments\n",
    "K = 0.01* Kcrit\n",
    "\n",
    "%store  Astored\n",
    "A=torch.tensor(Astored).to(torch.float32).to(device)\n",
    "L=torch.tensor(L).to(torch.float32).to(device)\n",
    "#check memory\n",
    "print(get_gpu_memory())\n",
    "#check coupling constant (to avoid Nan, inf)\n",
    "print(K)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e76a8c-1335-4d26-a2c0-c309c7a3e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save performance data\n",
    "global_perf=[]\n",
    "global_eperf=[]\n",
    "global_stperf=[]\n",
    "global_param=[]\n",
    "\n",
    "dist = torch.distributions.Normal(0, 1)\n",
    "#number of epochs\n",
    "iter = 5000\n",
    "#number of time steps\n",
    "t_size=32\n",
    "#total time    \n",
    "t_total=1\n",
    "\n",
    "ts_exp = torch.linspace(0, t_total, t_size).to(device)\n",
    "\n",
    "for layers in range(1,5):\n",
    "    \n",
    "    skip=False\n",
    "    #select strategy: tHNC, HNC, ENC\n",
    "    strategy=\"tHNC\"\n",
    "    #initialize model\n",
    "    SML= KuramotoHyperUniversal(batch_size, state_size, K,A, layers,skip,strategy,device).to(device)\n",
    " \n",
    "    #initialize optimizer\n",
    "    params=find_params(SML)\n",
    "    controller_optimizer = torch.optim.AdamW(params, lr=learning_rate,  weight_decay=0.0, amsgrad=False)\n",
    "    #initialize scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(controller_optimizer, gamma=gamma)\n",
    "    \n",
    "    #track performance\n",
    "    perf=[]\n",
    "    eperf=[]\n",
    "    stperf=[]\n",
    "    erpe=[]\n",
    "    orpam=[]\n",
    "\n",
    "    #initialize EMA model\n",
    "    ema_model = copy.deepcopy(SML)\n",
    "    ema_loss=0\n",
    "\n",
    "    score=1000000\n",
    " \n",
    "    for j in tqdm(range(iter)):\n",
    "        #sample initial conditions and parameters\n",
    "        freqs, ys_exp= sample_kuramoto(state_size,batch_size, dist, scale, scale_kin, device)\n",
    "\n",
    "        SML.freqs=freqs\n",
    "        #forward simulations\n",
    "        ys_tray = torchsde.sdeint_adjoint(SML, ys_exp, ts_exp,method='reversible_heun', dt=ts_exp[1]-ts_exp[0],\n",
    "                                     adjoint_method='adjoint_reversible_heun',)\n",
    "        #compute loss\n",
    "        loss, ste_loss, logtrick=loss_compute_kur(ys_tray, state_size ,batch_size,weight,A)\n",
    "        \n",
    "        print(\"order parameter\")\n",
    "        ord_param=order_param(ys_tray[-1,:,0:state_size],L)\n",
    "        print(ord_param)\n",
    "    \n",
    "        logtrick.backward()\n",
    "        print(\"total loss\")\n",
    "        print(loss)\n",
    " \n",
    "        #perform update of parameters\n",
    "        controller_optimizer.step()\n",
    "        controller_optimizer.zero_grad(set_to_none=True)\n",
    "        #perform scheduler step\n",
    "        scheduler.step()\n",
    "        #update EMA model\n",
    "        update_ema(SML, ema_model, alpha)\n",
    "        print(get_gpu_memory())\n",
    "        \n",
    "        #compute ema loss\n",
    "        if j==0:\n",
    "            ema_loss=loss.cpu().detach().numpy()  \n",
    "        else:\n",
    "            ema_loss=ema_loss*alpha+loss.cpu().detach().numpy()*(1-alpha)\n",
    "\n",
    "        #save the model\n",
    "        score= save_model(score, ema_loss, ema_model, SML, path,strategy,layers)\n",
    "\n",
    "        #append data\n",
    "        perf.append(loss.cpu().detach().numpy())\n",
    "        eperf.append(ema_loss)\n",
    "        stperf.append(ste_loss.cpu().detach().numpy())\n",
    "        orpam.append(ord_param.cpu().detach().numpy())\n",
    "        \n",
    "        #clear memory\n",
    "        gc.collect()        \n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "        #clear output\n",
    "        if j>100 and j%500==0:\n",
    "                clear_output(wait=False) \n",
    "        del ys_tray\n",
    "    #append data    \n",
    "    global_perf.append(perf)\n",
    "    global_eperf.append(eperf)\n",
    "    global_stperf.append(stperf)\n",
    "    global_param.append(orpam)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
