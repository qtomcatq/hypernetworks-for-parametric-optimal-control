{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a36c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "import control\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchsde\n",
    "from torch import nn, profiler, Tensor, nan, vmap\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..')) \n",
    "sys.path.append(project_root)  # Add folder_a to sys.path\n",
    "sys.path.append(os.path.join(project_root, '../..', 'Networks'))  # Add folder_b to sys.path\n",
    "\n",
    "from Networks.utils import get_gpu_memory, normal, ema_update,mass_append, update_ema, generate_system_canon, load_clean_state_dict, sample_linear, compute_gradnorm,save_model, loss_compute\n",
    "from helpers import   HyperCoeffsLinearControlStochasticLQRImpl,  find_params\n",
    "import psutil\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from adabelief_pytorch import AdaBelief\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if is_cuda else 'cpu'\n",
    "\n",
    "if not is_cuda:\n",
    "    print(\"Warning: CUDA not available; falling back to CPU but this is likely to be very slow.\")\n",
    "import gc\n",
    "path=os.path.abspath(os.getcwd())\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c426e2-b4ac-452a-864d-b22cbcf1e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#state size\n",
    "state_size=128\n",
    "#batch size\n",
    "batch_size=1024\n",
    "#weight\n",
    "weight=0.1\n",
    "#generate canonical A and B\n",
    "A,B=generate_system_canon(state_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffad5b-bce5-472e-a90d-aaa90319c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate\n",
    "learning_rate=0.001\n",
    "#EMA coefficient\n",
    "alpha=0.999\n",
    "#scheduler decay rate\n",
    "gamma=0.9999\n",
    "#total epochs\n",
    "iter=5000\n",
    "\n",
    "#store performance data\n",
    "global_perf=[]\n",
    "global_eperf=[]\n",
    "global_stperf=[]\n",
    "global_grad=[]\n",
    "\n",
    "#initialize dynamic matrices\n",
    "At=torch.tensor(A).float().to(device) \n",
    "Bt=torch.tensor(B).float().to(device) \n",
    "coremat=(At.unsqueeze(0)).repeat(batch_size,1,1)\n",
    "#choose strategy: HNC or ENC\n",
    "strategy=\"HNC\"\n",
    "\n",
    "#total time\n",
    "t_total=1\n",
    "t_size=int(32)\n",
    "ts_exp = torch.linspace(0, t_total, t_size).float().to(device)\n",
    "\n",
    "for layers in range(1,5):\n",
    "    #initialize model\n",
    "    SML= HyperCoeffsLinearControlStochasticLQRImpl(At,Bt,batch_size, device,layers,strategy).to(device) \n",
    "    \n",
    "    #initialize optimizer\n",
    "    params =  find_params(SML)    \n",
    "    controller_optimizer = AdaBelief(params, lr=learning_rate, eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = True, amsgrad=False)\n",
    "    #initialize scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(controller_optimizer, gamma=gamma)\n",
    "    \n",
    "    \n",
    "    SML.batch=batch_size\n",
    "              \n",
    "    score= 10000000\n",
    "    #initialize EMA model\n",
    "    ema_model = copy.deepcopy(SML)\n",
    "    \n",
    "                \n",
    "    ema_loss, ema_ener, ema_std, enr_std = (0.0,) * 4\n",
    "\n",
    "    eperf, stperf, eloss, stloss, emaperf, emastperf, emaener, emastener, gradnorms = ([] for _ in range(9))\n",
    "\n",
    "   \n",
    "    for j in tqdm(range(iter)):\n",
    "        #sample initial conditions and system parameters\n",
    "        amat, x0= sample_linear(dim,batch_size, device)\n",
    "       \n",
    "        ini_enr = torch.zeros(batch_size, 1).to(device)\n",
    "    \n",
    "        ys_exp = torch.cat((x0,ini_enr),dim=1).to(device)\n",
    "        \n",
    "        coremat[:,-1,:]=amat\n",
    "        SML.At= coremat\n",
    "        SML.Bt= Bt\n",
    "        SML.poly.amat=amat\n",
    "\n",
    "        #forward simulations\n",
    "        ys_tray = torchsde.sdeint_adjoint(SML, ys_exp, ts_exp,method='reversible_heun', dt=ts_exp[1]-ts_exp[0],\n",
    "                             adjoint_method='adjoint_reversible_heun',)\n",
    "        \n",
    "                \n",
    "        print(psutil.virtual_memory())\n",
    "        #calculate loss\n",
    "        lossvec, logtrick, stdvec, energy, stdener=loss_compute(ys_tray, state_size, weight) \n",
    "\n",
    "        print('total loss')\n",
    "        print(lossvec)\n",
    "        \n",
    "        logtrick.backward()\n",
    "        \n",
    "        total_sq_norm=compute_gradnorm(SML)    \n",
    "\n",
    "        print(\"grad norm\")\n",
    "        print(total_sq_norm)\n",
    "    \n",
    "        #update parameters\n",
    "        controller_optimizer.step()\n",
    "        controller_optimizer.zero_grad(set_to_none=True)\n",
    "        #scheduler step\n",
    "        scheduler.step()\n",
    "        #update EMA model\n",
    "        update_ema(SML, ema_model, alpha)\n",
    "        [ema_loss, ema_ener, enr_std, ema_std]=ema_update(j,alpha, lossvec,energy,stdener,stdvec,ema_loss,ema_ener,enr_std,ema_std)\n",
    "       \n",
    "        \n",
    "        print('ema loss')\n",
    "        print(ema_loss)\n",
    "        print('ema_ener')\n",
    "        print(ema_ener)\n",
    "        \n",
    "        [eperf, stperf, eloss, stloss, emaperf, emaener,emastperf,emastener, gradnorms]= mass_append(lossvec,stdvec,energy,stdener, batch_size, ema_loss, ema_ener, enr_std, ema_std, eperf, stperf, eloss, stloss, emaperf, emaener,emastperf,emastener,total_sq_norm, gradnorms)\n",
    "        #save model\n",
    "        score= save_model(score, ema_loss, ema_model, SML, path,strategy,layers)\n",
    "     \n",
    "        if lossvec>500:\n",
    "            break\n",
    "        \n",
    "        if j%200==0:\n",
    "            gc.collect()\n",
    "            clear_output(wait=True)\n",
    "        del lossvec, stdvec \n",
    "    global_stperf.append(stperf)    \n",
    "    global_perf.append(eperf)\n",
    "    global_eperf.append(emaperf)\n",
    "    global_grad.append(gradnorms)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
