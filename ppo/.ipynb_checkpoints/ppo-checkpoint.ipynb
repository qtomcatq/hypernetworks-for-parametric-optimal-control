{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f8bacc-a11f-4890-8a7a-3fd40e3f047a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 09:54:12.222482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-03 09:54:12.233632: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-03 09:54:12.236946: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-03 09:54:12.797256: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8413104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "import pdb\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, lax\n",
    "from torch.distributions.normal import Normal\n",
    "import os\n",
    "import sys\n",
    "path=os.path.abspath(os.getcwd())\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..')) \n",
    "sys.path.append(project_root)  # Add folder_a to sys.path\n",
    "sys.path.append(os.path.join(project_root, '..', 'Networks'))  # Add folder_b to sys.path\n",
    "import gc\n",
    "import equinox as eqx\n",
    "import optax\n",
    "from jax.random import PRNGKey\n",
    "import distrax\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from adabelief_pytorch import AdaBelief\n",
    "from kuramoto_env import activate_state, init_sim_params, heun_step, SimState, count_reward_batch, criticalK, order_param\n",
    "from policy import ActorCritic, compute_gae, ppo_update, find_params\n",
    "TOTAL_TIMESTEPS = 100_000\n",
    "STEPS_PER_TRAJECTORY = 32\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1024\n",
    "GAMMA = 0.999999\n",
    "GAE_LAMBDA = 0.999999\n",
    "CLIP_EPSILON = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "REWARD_SCALE = 1e-5\n",
    "ACTION_SCALE = 2000.0\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "noise_rms=32\n",
    "# --------------------------\n",
    "# PyTree dataclasses\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "dt = 1.0 / STEPS_PER_TRAJECTORY\n",
    "T = STEPS_PER_TRAJECTORY\n",
    "\n",
    "%store -r Astored\n",
    "\n",
    "A = Astored\n",
    "dim=A.shape[0]\n",
    "# compute critical K and Laplacian\n",
    "Kcrit, L = criticalK(A, dim, 5.0)\n",
    "K = 0.01 * Kcrit\n",
    "\n",
    "print(K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5aefb17-69a2-42cb-8e4d-291898031b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/10000 [00:01<5:20:04,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average order param\n",
      "0.97729415\n",
      "ema loss\n",
      "82.82687\n",
      "model saved\n",
      "ep rewards\n",
      "-37.6263\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.6263\n",
      "average order param\n",
      "0.9777042\n",
      "ema loss\n",
      "82.82027130126953\n",
      "model saved\n",
      "ep rewards\n",
      "-37.15973\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.1597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/10000 [00:02<52:00,  3.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average order param\n",
      "0.9777312\n",
      "ema loss\n",
      "82.81292737579345\n",
      "model saved\n",
      "ep rewards\n",
      "-36.98694\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -36.9869\n",
      "average order param\n",
      "0.97752726\n",
      "ema loss\n",
      "82.80926724941254\n",
      "model saved\n",
      "ep rewards\n",
      "-37.454044\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.4540\n",
      "average order param\n",
      "0.9775597\n",
      "ema loss\n",
      "82.80514118599262\n",
      "model saved\n",
      "ep rewards\n",
      "-37.590748\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.5907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/10000 [00:02<35:39,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average order param\n",
      "0.977435\n",
      "ema loss\n",
      "82.80280821382732\n",
      "model saved\n",
      "ep rewards\n",
      "-37.815826\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.8158\n",
      "average order param\n",
      "0.9774648\n",
      "ema loss\n",
      "82.79992336129578\n",
      "model saved\n",
      "ep rewards\n",
      "-37.093582\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.0936\n",
      "average order param\n",
      "0.9775175\n",
      "ema loss\n",
      "82.79608258848344\n",
      "model saved\n",
      "ep rewards\n",
      "-37.68789\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.6879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/10000 [00:02<22:53,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average order param\n",
      "0.97761667\n",
      "ema loss\n",
      "82.7910723598672\n",
      "model saved\n",
      "ep rewards\n",
      "-37.504463\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.5045\n",
      "average order param\n",
      "0.97739404\n",
      "ema loss\n",
      "82.78976363503075\n",
      "model saved\n",
      "ep rewards\n",
      "-37.920525\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.9205\n",
      "average order param\n",
      "0.9774753\n",
      "ema loss\n",
      "82.78723094692151\n",
      "model saved\n",
      "ep rewards\n",
      "-37.26523\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.2652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/10000 [00:02<19:42,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average order param\n",
      "0.97744834\n",
      "ema loss\n",
      "82.7848010635492\n",
      "model saved\n",
      "ep rewards\n",
      "-37.348305\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.3483\n",
      "average order param\n",
      "0.9774655\n",
      "ema loss\n",
      "82.78242078992089\n",
      "model saved\n",
      "ep rewards\n",
      "-37.55097\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.5510\n",
      "average order param\n",
      "0.9773683\n",
      "ema loss\n",
      "82.78124657049278\n",
      "model saved\n",
      "ep rewards\n",
      "-38.25946\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -38.2595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/10000 [00:03<16:21, 10.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average order param\n",
      "0.977604\n",
      "ema loss\n",
      "82.77687570751335\n",
      "model saved\n",
      "ep rewards\n",
      "-37.217518\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.2175\n",
      "average order param\n",
      "0.9773725\n",
      "ema loss\n",
      "82.77580710961543\n",
      "model saved\n",
      "ep rewards\n",
      "-37.407745\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.4077\n",
      "average order param\n",
      "0.9775093\n",
      "ema loss\n",
      "82.77269284457516\n",
      "model saved\n",
      "ep rewards\n",
      "-37.790047\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.7900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/10000 [00:03<15:31, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average order param\n",
      "0.9774425\n",
      "ema loss\n",
      "82.77031432755687\n",
      "model saved\n",
      "ep rewards\n",
      "-38.000557\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -38.0006\n",
      "average order param\n",
      "0.9775644\n",
      "ema loss\n",
      "82.76641358447134\n",
      "model saved\n",
      "ep rewards\n",
      "-37.251377\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.2514\n",
      "average order param\n",
      "0.9774035\n",
      "ema loss\n",
      "82.76523460035025\n",
      "model saved\n",
      "ep rewards\n",
      "-37.60225\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.6022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/10000 [00:03<14:24, 11.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average order param\n",
      "0.9774536\n",
      "ema loss\n",
      "82.7630360823778\n",
      "model saved\n",
      "ep rewards\n",
      "-38.08651\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -38.0865\n",
      "average order param\n",
      "0.9773453\n",
      "ema loss\n",
      "82.76231984758358\n",
      "model saved\n",
      "ep rewards\n",
      "-37.922165\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.9222\n",
      "average order param\n",
      "0.9774537\n",
      "ema loss\n",
      "82.759767556207\n",
      "model saved\n",
      "ep rewards\n",
      "-37.422318\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.4223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25/10000 [00:03<14:10, 11.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average order param\n",
      "0.9773666\n",
      "ema loss\n",
      "82.75874577988837\n",
      "model saved\n",
      "ep rewards\n",
      "-37.854042\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.8540\n",
      "average order param\n",
      "0.9774128\n",
      "ema loss\n",
      "82.75727203542496\n",
      "model saved\n",
      "ep rewards\n",
      "-37.96847\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.9685\n",
      "average order param\n",
      "0.977279\n",
      "ema loss\n",
      "82.7577415803992\n",
      "model saved\n",
      "ep rewards\n",
      "-37.626724\n",
      "grad norm\n",
      "0\n",
      "Steps: 0, Avg Reward: -37.6267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26/10000 [00:03<25:33,  6.51it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#number of iterations for each configuration\n",
    "totepochs=10000\n",
    "cost_global=[]\n",
    "cost_std_global=[]\n",
    "orparam_global=[]\n",
    "emaloss_global=[]\n",
    "\n",
    "alpha=0.995\n",
    "#omniscent = has access to both state and parameters, otherwise only parameters\n",
    "omniscent=True\n",
    "strategy='MLP'\n",
    "#weight\n",
    "R=0.0001\n",
    "#loop over different number of layers of actor and critic\n",
    "for layers in range(0,5):\n",
    "    for layer_c in range(0,5):\n",
    "        total_steps = 0\n",
    " \n",
    "        final_cost_array=[]\n",
    "        final_cost_array_std=[]\n",
    "        final_orparam=[]\n",
    "        final_ema_array=[]\n",
    "      \n",
    "        key = PRNGKey(0)\n",
    "        #initialize model and optimizer\n",
    "        model = ActorCritic(3*A.shape[0]+1, A.shape[0],layers, layer_c , omniscent,strategy).to(DEVICE)\n",
    "        parameters=find_params(model)\n",
    "        optimizer = AdaBelief(parameters, lr=LEARNING_RATE, eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = True, amsgrad=False)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "        ema_loss=0\n",
    "        #start training for chosen configuration\n",
    "        for epochs in tqdm(range(totepochs)):\n",
    "            rollout_obs = []\n",
    "            rollout_acts = []\n",
    "            rollout_log_probs = []\n",
    "            rollout_rewards = []\n",
    "            rollout_values = []\n",
    "            rollout_dones = []\n",
    "            \n",
    "            total_energy=0\n",
    "            energy_array=[]\n",
    "            cost_array=[]\n",
    "            orparam_array=[]\n",
    "\n",
    "            #initialize the environment\n",
    "            key, sub = jax.random.split(key)\n",
    "            state = activate_state(sub, BATCH_SIZE, dim)\n",
    "            key, sub = jax.random.split(key)\n",
    "            params = init_sim_params(sub, A, K, BATCH_SIZE)\n",
    "            freqs_obs=params.freqs\n",
    "        \n",
    "            episode_penalty=0\n",
    "            episode_rewards = []\n",
    "            episode_reward = 0\n",
    "            t=0\n",
    "   \n",
    "            score=1000000\n",
    "            #start episode\n",
    "            for _ in range(STEPS_PER_TRAJECTORY):\n",
    "                #observation vector \n",
    "                obs = jnp.concatenate([jnp.sin(state.N),jnp.cos(state.N),jnp.expand_dims(state.t*dt,1), freqs_obs],axis=1)\n",
    "                obs=torch.tensor(np.array(obs), device=DEVICE)\n",
    "                #action excecution\n",
    "                with torch.no_grad():\n",
    "            \n",
    "                    mean, std, value = model(obs)\n",
    "                    dist = Normal(mean, noise_rms*std*dt)\n",
    "                    action = dist.sample()\n",
    "                    log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "                #save the variables\n",
    "                rollout_obs.append(obs.clone())\n",
    "                rollout_acts.append(action)\n",
    "                rollout_log_probs.append(log_prob)\n",
    "                rollout_values.append(value)\n",
    "        \n",
    "                #update the system's state and evaluate rewards\n",
    "                N_next = heun_step(state.N, jnp.array(np.array(action.cpu())), params, dt)  # (num_envs, dim)\n",
    "                next_state = SimState(N=N_next,t=state.t+1)\n",
    "                reward, r1, energy,penalty = count_reward_batch(state.N, N_next, jnp.array(np.array(action.cpu())), params.A, R, dt)\n",
    "                \n",
    "               \n",
    "                rollout_rewards.append(torch.tensor(np.array(reward), dtype=torch.float32, device=DEVICE))\n",
    "                \n",
    "                total_energy+=np.array(penalty)\n",
    "                episode_reward += np.array(reward)\n",
    "             \n",
    "                episode_penalty+=np.array(penalty)\n",
    "                t += 1\n",
    "\n",
    "                #finish if time limit violated\n",
    "                if t<STEPS_PER_TRAJECTORY:\n",
    "                    done=False\n",
    "                else:\n",
    "                    done=True\n",
    "                    order = order_param(N_next, L)\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    episode_reward = 0\n",
    "        \n",
    "                    energy_array.append(total_energy) \n",
    "                    cost_array.append(np.array(r1))\n",
    "                    orparam_array.append(np.array(order))\n",
    "                    total_energy=0\n",
    "                    \n",
    "                rollout_dones.append(torch.tensor(np.array(done), dtype=torch.float32, device=DEVICE))    \n",
    "                #update the state\n",
    "                state=next_state\n",
    "                \n",
    "            print(\"average order param\")\n",
    "            print(np.mean(orparam_array[0]))\n",
    "            mean_loss=np.mean(cost_array[0]+energy_array[0])\n",
    "            if epochs==0:\n",
    "                ema_loss=mean_loss        \n",
    "            else:\n",
    "                ema_loss=ema_loss*alpha+mean_loss*(1-alpha)\n",
    "                \n",
    "            print(\"ema loss\")\n",
    "            print(ema_loss)\n",
    "            #checkpoint the model\n",
    "            if score>(ema_loss):\n",
    "                score=(ema_loss)\n",
    "                torch.save(model.state_dict(), path+\"/ppo\" + str(layers)+str(layer_c)+ \".pth\")\n",
    "                print('model saved')\n",
    "              \n",
    "            #save the data\n",
    "            final_cost_array.append(mean_loss)\n",
    "            final_orparam.append(np.mean(orparam_array[0]))\n",
    "            final_ema_array.append(np.mean(ema_loss))\n",
    "            final_cost_array_std.append(np.std(cost_array[0]+energy_array[0])/np.sqrt(len(cost_array[0]))) \n",
    "\n",
    "            (rollout_obs, rollout_acts, rollout_log_probs, rollout_rewards, rollout_values, rollout_dones,\n",
    "            ) = map(torch.stack, ( rollout_obs, rollout_acts, rollout_log_probs, rollout_rewards, rollout_values, rollout_dones, ))\n",
    "    \n",
    "            print(\"ep rewards\")\n",
    "            print(np.mean(episode_rewards))\n",
    "            # Compute next value for GAE\n",
    "            with torch.no_grad():\n",
    "                _, _, next_value = model(obs)\n",
    "          \n",
    "            # Compute advantages and returns\n",
    "            advantages, returns = compute_gae(\n",
    "                rollout_rewards, rollout_values, next_value, rollout_dones, GAMMA, GAE_LAMBDA\n",
    "            )\n",
    "            epochs+=1\n",
    "          \n",
    "            #update actor and critic parameters\n",
    "            ppo_update(\n",
    "                model, optimizer, scheduler,rollout_obs, rollout_acts, rollout_log_probs,\n",
    "                advantages, returns, CLIP_EPSILON, BATCH_SIZE\n",
    "            )\n",
    "             # Print progress\n",
    "            if episode_rewards:\n",
    "                print(f\"Steps: {total_steps}, Avg Reward: {np.mean(episode_rewards[-100:]):.4f}\")\n",
    "    \n",
    "            if epochs%500==0:\n",
    "                clear_output(wait=False)\n",
    "            \n",
    "            del rollout_obs, rollout_acts, rollout_log_probs, rollout_rewards, rollout_values, rollout_dones, advantages, returns\n",
    "        #save the data\n",
    "        cost_global.append(final_cost_array)\n",
    "        cost_std_global.append(final_cost_array_std)\n",
    "        orparam_global.append(final_orparam)\n",
    "        emaloss_global.append(final_ema_array)\n",
    "        gc.collect()\n",
    "            \n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5234dbd-6b6a-4d8c-83fb-af0d88f2dd22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
